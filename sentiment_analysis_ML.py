# -*- coding: utf-8 -*-
"""Sentiment NB_RF_SVM plus BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h54RXyIujxlXDtmg1I5k4L3bJRJhSaZd
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install transformers

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

import pandas as pd
import numpy as np
import argparse
import collections
import csv
import json

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import re
import string
!pip install emoji
import emoji
from pprint import pprint

import os

import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns
import missingno as msno
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.externals import joblib
from sklearn.metrics import f1_score, accuracy_score
import gensim

# %matplotlib inline
inline_rc = dict(mpl.rcParams)

from nltk.corpus import stopwords
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS

from nltk.tokenize import word_tokenize
from nltk import FreqDist
from nltk.stem import PorterStemmer


import nltk
nltk.download('stopwords')
nltk.download('punkt')


from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# %load_ext autoreload
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob

import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
from tqdm import tqdm
tqdm.pandas()

from sklearn.feature_extraction.text import TfidfVectorizer

import time
from sklearn import svm
from sklearn.metrics import classification_report

from sklearn.utils import shuffle

#Mount Drive
from google.colab import drive
drive.mount('/content/MyDrive', force_remount=True)

df = pd.read_csv('/content/MyDrive/MyDrive/Colab Notebooks/SentimentAnalysis/ML-Project_1/data/yelp_reviews_train.csv')

df['truth'] = df['stars']
df = df[['truth','text']]
df['Content'] = df['text']
df = df[['Content','truth']]
df['Label'] = df['truth'] 
#print(df.truth)
df.loc[(df.truth < 3), 'Label'] = 'neg'
df.loc[(df.truth > 3), 'Label'] = 'pos'
df.drop(df[df['truth']==3].index, inplace= True)
df = df[['Content','Label']]
df = df.reset_index(drop=True)
df

x=df['Label'].value_counts()
x=x.sort_index()
plt.figure(figsize=(8,4))
ax= sns.barplot(x.index, x.values, alpha=1)
plt.title("Star Rating Distribution")
plt.ylabel('# of businesses', fontsize=12)
plt.xlabel('Star Ratings ', fontsize=12)

class CleanText(BaseEstimator, TransformerMixin):
    def remove_mentions(self, input_text):
        return re.sub(r'@\w+', '', input_text)
    
    def remove_urls(self, input_text):
        return re.sub(r'http.?://[^\s]+[\s]?', '', input_text)
    
    def emoji_oneword(self, input_text):
        return input_text.replace('_','')
    
    def remove_punctuation(self, input_text):
        punct = string.punctuation
        trantab = str.maketrans(punct, len(punct)*' ') 
        return input_text.translate(trantab)
    def remove_digits(self, input_text):
        return re.sub('\d+', '', input_text)
    
    def to_lower(self, input_text):
        return input_text.lower()
    
    def remove_stopwords(self, input_text):
        stopwords_list = stopwords.words('english')
        whitelist = ["n't", "not", "no"]
        words = input_text.split() 
        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] 
        return " ".join(clean_words) 
    
    def stemming(self, input_text):
        porter = PorterStemmer()
        words = input_text.split() 
        stemmed_words = [porter.stem(word) for word in words]
        return " ".join(stemmed_words)
    
    def fit(self, X, y=None, **fit_params):
        return self
    
    def transform(self, X, **transform_params):
        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)
        return clean_X

ct = CleanText()
sr_clean = ct.fit_transform(df.Content)
sr = sr_clean.to_frame()
df['Content'] = sr['Content']
trainData_df = df

df2 = pd.read_csv('/content/MyDrive/MyDrive/Colab Notebooks/SentimentAnalysis/ML-Project_1/data/yelp_reviews_test.csv')


df2['truth'] = df2['stars']
df2 = df2[['truth','text']]
df2['Content'] = df2['text']
df2 = df2[['Content','truth']]
df2['Label'] = df2['truth'] 

df2.loc[(df2.truth < 3), 'Label'] = 'neg'
df2.loc[(df2.truth > 3), 'Label'] = 'pos'
df2.drop(df2[df2['truth']==3].index, inplace= True)
df2 = df2[['Content','Label']]
df2 = df2.reset_index(drop=True)

ct = CleanText()
sr_clean = ct.fit_transform(df2.Content)
sr = sr_clean.to_frame()
df2['Content'] = sr['Content']
testData = df2.head(2000)



"""#Random Forest, Naive Bayes, SVM"""

n = [200,500,1000,5000,10000]
for i in n:
  df3 = df.head(i)
  trainData = pd.concat([df3],axis=0)
  trainData = trainData.reset_index(drop=True)
  vectorizer = TfidfVectorizer(min_df = 1,
                              max_df = 0.8,
                              sublinear_tf = True,
                              use_idf = True)

  train_vectors = vectorizer.fit_transform(trainData['Content'])
  test_vectors = vectorizer.transform(testData['Content'])

  print(i, "RandomForest ============")
  clf_rand  = RandomForestClassifier(n_estimators=500, criterion='entropy')
  clf_rand.fit(train_vectors, trainData['Label'])
  prediction_randForest = clf_rand.predict(test_vectors)

  report_rand = classification_report(testData['Label'], prediction_randForest, output_dict=True)
  print("precision =",(report_rand['pos']['precision'] + report_rand['neg']['precision'])/2)
  print("recall =",(report_rand['pos']['recall'] + report_rand['neg']['recall'])/2)
  print("f1-macro =",(report_rand['pos']['f1-score'] + report_rand['neg']['f1-score'])/2)

  print("Naive Bayes ============")
  clf_naive  = MultinomialNB()
  clf_naive.fit(train_vectors, trainData['Label'])
  prediction_naiveBayes = clf_naive.predict(test_vectors)

  report_naive = classification_report(testData['Label'], prediction_naiveBayes, output_dict=True)
  print("precision =",(report_naive['pos']['precision'] + report_naive['neg']['precision'])/2)
  print("recall =",(report_naive['pos']['recall'] + report_naive['neg']['recall'])/2)
  print("f1-macro =",(report_naive['pos']['f1-score'] + report_naive['neg']['f1-score'])/2)

  print("SVM ============")
  classifier_linear = svm.SVC(kernel='linear')
  classifier_linear.fit(train_vectors, trainData['Label'])
  prediction_linear = classifier_linear.predict(test_vectors)
  report_svm = classification_report(testData['Label'], prediction_linear, output_dict=True)

  print("precision =",(report_svm['pos']['precision'] + report_svm['neg']['precision'])/2)
  print("recall =",(report_svm['pos']['recall'] + report_svm['neg']['recall'])/2)
  print("f1-macro =",(report_svm['pos']['f1-score'] + report_svm['neg']['f1-score'])/2)

"""# FOR BERT"""

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

yelp_train_df = pd.read_csv('/content/MyDrive/MyDrive/Colab Notebooks/SentimentAnalysis/ML-Project_1/data/yelp_reviews_train.csv')

yelp_test_df = pd.read_csv('/content/MyDrive/MyDrive/Colab Notebooks/SentimentAnalysis/ML-Project_1/data/yelp_reviews_test.csv')
#yelp_train_df.head()

def getNumericLabel(sentiment) :
  if sentiment == 'neg':
    return  0 
  else:
    return 1

train  = yelp_train_df
train['sentiment'] = train['Label'].apply(getNumericLabel)
train['DATA_COLUMN'] = yelp_train_df['text']
train['LABEL_COLUMN'] = train['sentiment']
train = train[['DATA_COLUMN','LABEL_COLUMN']]
test = yelp_test_df
test['sentiment'] = test['Label'].apply(getNumericLabel)
test['DATA_COLUMN'] = yelp_test_df['text']
test['LABEL_COLUMN'] = test['sentiment']

train = train.head(10000)

def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): 
  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)

  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)
  
  return train_InputExamples, validation_InputExamples

  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, 
                                                                           test, 
                                                                           'DATA_COLUMN', 
                                                                           'LABEL_COLUMN')
  
def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):
    features = [] # -> will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text_a,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) > max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length
            truncation=True
        )

        input_ids, token_type_ids, attention_mask = (input_dict["input_ids"],
            input_dict["token_type_ids"], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    "input_ids": f.input_ids,
                    "attention_mask": f.attention_mask,
                    "token_type_ids": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({"input_ids": tf.int32, "attention_mask": tf.int32, "token_type_ids": tf.int32}, tf.int64),
        (
            {
                "input_ids": tf.TensorShape([None]),
                "attention_mask": tf.TensorShape([None]),
                "token_type_ids": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )


DATA_COLUMN = 'DATA_COLUMN'
LABEL_COLUMN = 'LABEL_COLUMN'

train_InputExamples, validation_InputExamples = convert_data_to_examples(train[:10000], test[:2000], DATA_COLUMN, LABEL_COLUMN)

train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)
train_data = train_data.shuffle(100).batch(32).repeat(2)

validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)
validation_data = validation_data.batch(32)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.mean_squared_error, 
                       tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

m = model.fit(train_data, epochs=2, validation_data=validation_data)

# Plot training & validation accuracy values
plt.plot(m.history['accuracy'])
plt.plot(m.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

!mkdir -p saved_model
model.save('saved_model/my_model')

new_model = tf.keras.models.load_model('saved_model/my_model')

new_model.summary()

test_result = test[:2000]
labels = ['Negative','Positive']


def BERT_Predict(review):
  tf_batch = tokenizer(review, max_length=128, padding=True, truncation=True, return_tensors='tf')
  tf_outputs = model(tf_batch)
  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)
  label = tf.argmax(tf_predictions, axis=1)
  label = label.numpy()
  #print(" >> ", review, ":", labels[label[0]], "\n")

  return label[0];

test_result['PRED_LABEL_COLUMN'] = test_result['DATA_COLUMN'].apply(BERT_Predict)
#test_result

print(classification_report(test_result['LABEL_COLUMN'], test_result['PRED_LABEL_COLUMN']))